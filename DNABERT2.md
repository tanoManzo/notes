Zhou et al.,

## Abstract 

Decoding the linguistic intricacies of the genome pre-trained foundational models such as [[DNABERT pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome]] and [[The Nucleotide Transformer - Building and Evaluating Robust Foundation Models for Human Genomics]] have made significant strides in this area.


Existing works have largely hinged on k-mer, fixed-length permutations of A, T, C, and G, as the token of the genome language due to its simplicity. However, we argue that the computation and sample inefficiencies introduced by k-mer tokenization are primary obstacles in developing large genome foundational models.