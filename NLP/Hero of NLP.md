[[NLP]]

**Chris Manning** (Stanford):  
- Linguistics, AI, mentioned Chomsky (human cannot learn language only from the data), 
- Transformer Architecture, Attention (soft tree structure), what transformers learn during the training (e.g., co-reference facts). 
- Paper on Neural Machine Translation (foundation of NLP, Attention, probabilistic model technique, etc.).
- Language models give you probability distribution of a sequence of words.
- Less attention to the syntax, more attention to the data. 
- Attention-based model, at any point in the sequence, you can calculate out a connection to other words. Use this Attention you can create a vector, which will help for the next. Instead of to keep all the sequence, you only keep these vectors to check the Attention for that word (you look back).
- Bi


