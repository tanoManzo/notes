[[NLP]]

**Chris Manning** (Stanford):  
- Linguistics, AI, mentioned Chomsky (human cannot learn language only from the data), 
- Transformer Architecture, Attention (soft tree structure), what transformers learn during the training (e.g., co-reference facts). 
- Paper on Neural Machine Translation (foundation of NLP, Attention, probabilistic model technique, etc.).
- Language models give you probability distribution of a sequence of words.
- Less attention to the syntax, more attention to the data. 


