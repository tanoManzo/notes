The transformer can be largely divided into two mechanisms: First, is an encoding block that compresses information regarding correlations between every word within the input sentence and converts the information into a latent representation. Second is a decoding block which outputs the probability of occurrence for each word in consideration of both the information sent from the encoder (reference sentence structure) and the input from the decoder (input 61 sentence). Recently, the focus has shifted towards improving QG prediction performance for each model through PTM incorporating the basic encoder-decoder block of the transformer. Bidirectional encoder representations from transformer (BERT) is configured to enable attention in two directions rather than one by using an encoder block. BERT is highly suitable for extracting the meaning of sentences because more information can be reflected during the encoding process . Generative pre-training (GPT) uses only the decoder block in the transformer structure to learn to predict the next word when a new word is given.  As a result, contemporary research is being undertaken utilizing an ensemble model based on the attention mechanism. Unlike BERT or GPT, which uses either the encoder block or decoder block of the transformer but not both, the text-to-text transfer transformer (T5) model simultaneously uses the encoder block and the decoder block, hence being adequate for application to all NLP tasks. 

seIn specific, when a representative question is selected among questions extracted from multiple NQG models of high similarity, a bias occurs due to high redundancy while training the chatbot algorithm. The issue however can be overcome using the proposed method. Utilization of the soft voting method for questions generated between each NQG model ensures a high similarity with the input text, low correlation between the generated questions, and concomitantly increases the number of generated questions, hence being suitable for constructing a PTM learning dataset. To do so, the average of the Bilingual Evaluation Understudy (BLEU) and Recall-Oriented Understudy for Gisting Evaluation (ROUGE) scores for the questions generated from each NQG model are selected as weights [24,25].


1. Fig  included in the mentioned page
2. motivation unclear 
3. intro should reduce sota and focus more on the motivation
4. T5 model should be introduced better before line 113
5. Section 3 should be highlighted that it is the author contribution. In the way it is written, it looks part of the sota. Section 3 can be subdivided in subsections to enhance readability. 
6. The approach should be motivated, why only BLUE and ROUGE, and which version. How the accuracy weights are evaluated. The paragraph 161-173 can be improved.
7. Section 3 is a mix of architecture description, sota, algorithm used and results. 
8. Table 2 should be improved. Bold the main results, please show the p-value, etc. Same reasoning for Tabke 3
