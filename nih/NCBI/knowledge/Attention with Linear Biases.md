Let q<sub>i</sub> define the i-th query in the input sequence of length L and K defines the key matrix, the attention score of query i is calculated as: softmax(q<sub>i</sub>K+m∗[−(i−1), ..., −2, −1, 0, −1, −2, ..., −(L−1−i)]), where m is a fixed head-specific constant. ALiBi used a geometric sequence (i.e., 1/2<sup>1</sup> , 1/2<sup>2</sup> , ..., 1/2<sup>n</sup>) of different m to each attention head. Intuitively, ALiBi increasingly penalizes attention scores between key-query pairs as their distances increase, and m determines the penalty rate. Replacing learned position embedding with ALiBi allows DNABERT-2 to effectively handle arbitrarily long sequences during fine-tuning and inference despite being pre-trained on relatively short sequences.

