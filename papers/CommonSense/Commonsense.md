
### Commonsense Knowledge Transfer for Pre-trained Language Models


by Wangchunshu Zhou, Ronan Le Bras, and Yejin Choi
<!-- element style="font-size: 24px"-->

---

LLMs do not possess deep semantic understanding or world knowledge like humans do

(Bender and Koller, 2020)
<!-- element style="font-size: 24px"-->

---
##### Two distinct lines of research

- <!-- element style="font-size: 28px"--> Incorporate commonsense knowledge graph for commonsense reasoning[^1]
-  <!-- element style="font-size: 28px"--> Inject commonsense knowledge into the parameters of pretrained models[^2]



<br />
<br />

```

1. (Lin et al., 2019; Liu et al., 2021; Cui and Chen, 2021)  2. (Li et al., 2019; Zhou et al., 2021; Klein and Nabi, 2021)
<!-- element style="font-size: 16px"-->

---
Here's a first footnote [^1] and here's the second number footnote [^2]. We can also insert a third number footnote [^3]. There can be multiple footnotes in a document.   
[^1]: This is the first number footnote.  
[^2]: This is the first number footnote.  
[^3]: This is the first number footnote.  

---
#### First Slide

![[Pasted image 20230627162059.png]]
---
Second
---