### Commonsense Knowledge Transfer for Pre-trained Language Models

by Wangchunshu Zhou, Ronan Le Bras, and Yejin Choi

---

LLMs are limited to exploiting's the surface form of human language, and the lack of grounded supervision calls into question how well these representations can ever capture meaning[^1]
[^1]: (Bender and Koller, 2020)
---
#### First Slide

![[Pasted image 20230627162059.png]]
---
Second
---