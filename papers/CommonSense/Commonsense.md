
### Commonsense Knowledge Transfer for Pre-trained Language Models


by Wangchunshu Zhou, Ronan Le Bras, and Yejin Choi
<!-- element style="font-size: 24px"-->

---

LLMs do not possess deep semantic understanding or world knowledge like humans do

(Bender and Koller, 2020)
<!-- element style="font-size: 24px"-->

---
##### Two distinct lines of research

<!-- element style="font-size: 24px"-->
- Incorporate external commonsense knowledge graph for commonsense reasoning[^1]
-  Inject commonsense knowledge into the parameters of pretrained models[^2] 


(Lin et al., 2019; Liu et al., 2021; Cui and Chen, 2021)
<!-- element style="font-size: 20px"-->
[2]: (Li et al., 2019; Zhou et al., 2021; Klein and Nabi, 2021)
<!-- element style="font-size: 20px"-->

---
#### First Slide

![[Pasted image 20230627162059.png]]
---
Second
---