### Commonsense Knowledge Transfer for Pre-trained Language Models

by Wangchunshu Zhou, Ronan Le Bras, and Yejin Choi

---
LLMs do not possess deep semantic understanding or world knowledge like humans do

(Bender and Koller, 2020)

---
##### Two distinct lines of research
1. Incorporate external commonsense knowledge graph for commonsense reasoning[^1] (Lin et al., 2019; Liu et al., 2021; Cui and Chen, 2021). 
2. **Inject commonsense knowledge into the parameters of pretrained models (Li et al., 2019; Zhou et al., 2021; Klein and Nabi, 2021).**
___
Two distinct lines of research focus on improving commonsense reasoning ability of pre-trained language models.

1. Incorporate external commonsense knowledge graph for commonsense reasoning (Lin et al., 2019; Liu et al., 2021; Cui and Chen, 2021). 
2. **Inject commonsense knowledge into the parameters of pretrained models (Li et al., 2019; Zhou et al., 2021; Klein and Nabi, 2021).**

%% comment %%
---
#### First Slide

![[Pasted image 20230627162059.png]]
---
Second
---