
### Commonsense Knowledge Transfer for Pre-trained Language Models


by Wangchunshu Zhou, Ronan Le Bras, and Yejin Choi
<!-- element style="font-size: 24px"-->

---

LLMs do not possess deep semantic understanding or world knowledge like humans do

(Bender and Koller, 2020)
<!-- element style="font-size: 24px"-->

---
##### Inject commonsense knowledge into the parameters of pre-trained models



---
##### Commonsense Knowledge Graph: a semi-structured way for representing commonsense concepts

![[Pasted image 20230704165532.png]]

(Sap et al., 2020, ATOMIC)
<!-- element style="font-size: 24px"-->

---


---
#### First Slide

![[Pasted image 20230627162059.png]]
---
Second
---